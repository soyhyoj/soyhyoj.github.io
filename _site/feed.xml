<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>HYOLAB</title>
    <description>Used to study brain. Now analyzing behavior.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 13 Jan 2021 16:43:14 +0100</pubDate>
    <lastBuildDate>Wed, 13 Jan 2021 16:43:14 +0100</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>What's on an aspiring data scientist's bookmarks</title>
        <description>&lt;p&gt;Here are my favorite free online studying materials and career prep tools which are digestible for the newbies building a data science career (including myself).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/bookmarks.jpg&quot; alt=&quot;Photo by Antonio DiCaterina on Unsplash&quot; class=&quot;img-responsive&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;statistics&quot;&gt;Statistics&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://greenteapress.com/wp/think-stats-2e/&quot;&gt;Think Stats 2e&lt;/a&gt; - Book&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://greenteapress.com/wp/think-bayes/&quot;&gt;Think Bayes&lt;/a&gt; - Book&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ocw.jhsph.edu/index.cfm/go/viewCourse/course/IntroBiostats/coursePage/schedule/&quot;&gt;Introduction to Biostatistics&lt;/a&gt; offered by The Johns Hopkins University - Lectures
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;python--machine-learning&quot;&gt;Python + Machine Learning&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://greenteapress.com/wp/think-python-2e/&quot;&gt;Think Python 2e&lt;/a&gt; - Book&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jakevdp.github.io/PythonDataScienceHandbook/&quot;&gt;Python Data Science Handbook&lt;/a&gt; - Book&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/#0&quot;&gt;TensorFlow, Keras and deep learning, without a PhD&lt;/a&gt; - Book&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In youtube, there are countless channels dedicated to the topic. My frequently visited sites are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&quot;&gt;3Blue1Brown&lt;/a&gt; - Cool visualization helps. Neural network series is my favorite.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/channel/UC8butISFwT-Wl7EV0hUK0BQ&quot;&gt;freeCodeCamp&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/c/DataUmbrella&quot;&gt;Data Umbrella&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/user/PyDataTV/playlists&quot;&gt;PyData&lt;/a&gt;’s local community meetups
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;open-data-sources&quot;&gt;Open data sources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/&quot;&gt;Kaggle&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.drivendata.org/&quot;&gt;DRIVENDATA&lt;/a&gt; - Kaggle-like website providing data for social good&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/rfordatascience/tidytuesday&quot;&gt;TidyTuesday&lt;/a&gt; - Open datasets provided by R community&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://data.humdata.org/&quot;&gt;The Humanitarian Data Exchange&lt;/a&gt; - Humanitarian database provided by &lt;a href=&quot;https://www.unocha.org/&quot;&gt;OCHA&lt;/a&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;career-advice&quot;&gt;Career advice&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://podcast.bestbook.cool/&quot;&gt;Build a Career in Data Science Podcast&lt;/a&gt; - Podcast&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://veekaybee.github.io/2019/02/13/data-science-is-different/&quot;&gt;Data science is different now&lt;/a&gt; - Blog post&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jKScXsJ5lGI&amp;amp;t=3637s&quot;&gt;Advancing Your Data Science Career During the Pandemic&lt;/a&gt; - Video&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/BecomingDataSci&quot;&gt;@BecomingDataSci&lt;/a&gt; on twitter and her &lt;a href=&quot;http://www.datasciguide.com/&quot;&gt;collection of DS learning materials&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.datahelpers.org/&quot;&gt;Data Helpers&lt;/a&gt; - A list of data professionals who are willing to help newcomers
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cv--résumé&quot;&gt;CV / Résumé&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.overleaf.com/&quot;&gt;Overleaf&lt;/a&gt; - Great for CV and cover letter; Easy of use &amp;amp; free templates available&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pages.github.com/&quot;&gt;GitHub Pages&lt;/a&gt; - To build a personalized website for portfolio
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 08 Jan 2021 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/2021/useful-ds-resources/</link>
        <guid isPermaLink="true">http://localhost:4000/2021/useful-ds-resources/</guid>
        
        <category>Data Science</category>
        
        <category>Data Analytics</category>
        
        
        <category>Resources</category>
        
      </item>
    
      <item>
        <title>WITH clause</title>
        <description>&lt;p&gt;Technical tests given during interview processes have been real boosting materials that led me to look for the better solutions to answer the questions. One of the querying techniques I enjoyed using recently is the &lt;strong&gt;WITH&lt;/strong&gt; clause. It is used for creating subquery blocks which are known as &lt;a href=&quot;https://docs.oracle.com/cd/E17952_01/mysql-8.0-en/with.html&quot;&gt;Common Table Expressions (CTE).&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This method is an elegant solution when you need to retrieve data and &lt;strong&gt;produce temporary results (e.g. transformed data) that can be referred to multiple times within a statement&lt;/strong&gt;. Also, it enhances the readability of subqueries when used in a complex SQL statement.&lt;/p&gt;

&lt;p&gt;Here is an example of a query including CTE from my recent &lt;a href=&quot;https://github.com/soyhyoj/GeospatialAnalysis_NYtaxi&quot;&gt;geospatial analysis&lt;/a&gt;.:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with
pickups as
(
select
	tpep_pickup_datetime as pickup_time,
	ST_SetSRID(ST_MakePoint(pickup_longitude, pickup_latitude), 4326)
	as pickup_point
from taxi
)

select pickups.*, census.geoid
from pickups, census_blocks as census
where ST_Contains(census.geometry, pickups.pickup_point)
LIMIT 10000;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;As a first step, &lt;em&gt;tpep_pickup_datetime&lt;/em&gt;, &lt;em&gt;pickup_longitude&lt;/em&gt;, and &lt;em&gt;pickup_latitude&lt;/em&gt; columns from a table called &lt;em&gt;taxi&lt;/em&gt; were retrieved from a PostGIS server.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A temporary geometry column called &lt;em&gt;pickup_point&lt;/em&gt; was created by combining &lt;em&gt;pickup_longitude&lt;/em&gt;, and &lt;em&gt;pickup_latitude&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The result of this subquery was named as &lt;em&gt;pickups&lt;/em&gt; and it was referred in the following block to produce a final result.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Multiple &lt;strong&gt;WITH&lt;/strong&gt; clauses can be embedded in a single query. The next example is a part of the queries to analyze a series of message records and it contains two &lt;strong&gt;WITH&lt;/strong&gt; clauses.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with
first_messages as
(
select
	order_id,
	message_sent_time,
	case when from_id = courier_id then 'Courier'
	else 'Customer'
	end as message_sender
from customer_courier_chat_messages
where message_sent_time in
	(
	select min(message_sent_time)
	from customer_courier_chat_messages
	group by order_id
	)
),

second_messages as
(
with message_order as
	(
	select
		order_id,
		message_sent_time,
		rank() over (partition by order_id
			    order by message_sent_time) as message_n
	from customer_courier_chat_messages
	)
select
	order_id,
	message_sent_time
from message_order
where message_n = 2
)

select  
	o.order_id,
	o.city_code,
	fm.message_sender as first_message_sender,
	fm.message_sent_time as first_message_time,
	EXTRACT(EPOCH from(sm.message_sent_time - fm.message_sent_time))
        as response_time,
into table customer_courier_conversations
from orders as o
	left join first_messages as fm
		on o.order_id = fm.order_id
	left join second_messages as sm
		on o.order_id = sm.order_id
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the first &lt;strong&gt;WITH&lt;/strong&gt; clause, the sender and time of the first message was retrieved temporally as &lt;em&gt;first_message&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the second &lt;strong&gt;WITH&lt;/strong&gt; clause, the time when the second message was sent was retrieved by numbering the order of conversation using &lt;strong&gt;ranking&lt;/strong&gt; method and &lt;strong&gt;window&lt;/strong&gt; function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The final result of the SQL statement was saved as a new table called &lt;em&gt;customer_courier_conversations&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In summary, &lt;strong&gt;CTEs&lt;/strong&gt; make it easier to understand subqueries by block separation. And by giving different names to the temporal results, they can be retrieved several times inside a SQL statement.&lt;/p&gt;
</description>
        <pubDate>Thu, 19 Nov 2020 00:00:00 +0100</pubDate>
        <link>http://localhost:4000/2020/with-clause/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/with-clause/</guid>
        
        <category>Job search</category>
        
        <category>SQL</category>
        
        <category>Database</category>
        
        
        <category>Database</category>
        
      </item>
    
      <item>
        <title>Garbage in, garbage out: Dealing with outliers</title>
        <description>&lt;p&gt;Recently I was asked to justify &lt;strong&gt;my choice of leaving outliers -instead of removing them-&lt;/strong&gt; during a technical interview for a data scientist position. The interview was based on a pre-handed take-home test where I provided a time series forecast related to a specific business case.&lt;/p&gt;

&lt;p&gt;Among the numerous questions I got from two interviewers over an hour, the one about outliers raised me a particular interest. Despite the confidence in my rationale built on a substantial EDA, their response was cluing me that they were clearly not happy with the way I dealt with the outliers. According to the on-site feedback, I had to &lt;strong&gt;exclude those values from the data before fitting a predictive algorithm.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To determine whether to retain, eliminate, or replace detected outliers, one should take into account not only the source/type of the data but also the cause/purpose of the preprocessing. This time I obviously made a naive mistake by betraying the basic principle of computer science: &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Garbage_in,_garbage_out&quot;&gt;Garbage in, garbage out&lt;/a&gt;&lt;/strong&gt;. In my defense, there was some transition shock.&lt;/p&gt;

&lt;p&gt;The confusion mainly came from the fact that the outliers presented in this particular assignment were &lt;strong&gt;correctly measured&lt;/strong&gt; out-of-range values. To add more context, I have been trained to collect and analyze experimental data in biology labs for years. In basic science research, what’s important is what you &lt;strong&gt;observe&lt;/strong&gt;, not what you &lt;strong&gt;expect&lt;/strong&gt; from the data. Since an extreme value itself is a natural event however rare its occurence, it would be unethical to intentionally exclude a part of the data from the rest &lt;a href=&quot;https://bolt.mph.ufl.edu/6050-6052/unit-1/one-quantitative-variable-introduction/understanding-outliers/&quot;&gt;without obvious doubt in its acquisition (e.g. failed to control conditions, fundamental errors in measurements).&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There is no more dreadful and disgusting thing than ‘data manipulation (=fabrication)’ in life science research and selectively choosing which sample to include for statistical testing is one. Thus I was naturally prone to be conservative of some of the data preprocessing techniques (imputing missing values, removing null records, and excluding outliers) and that was clearly a bad practice in machine learning. &lt;a href=&quot;http://greenteapress.com/thinkstats2/html/thinkstats2003.html#sec21&quot;&gt;Handling outliers can depend on ‘domain knowledge’&lt;/a&gt;. It’s what I’ve overlooked in this recent task by naively applying the rule I was familiar with (‘Do not exclude!’) to the data.&lt;/p&gt;

&lt;p&gt;Predictive model performance can be negatively impacted by the presence of outliers in the training data. These models, such as linear regression, would require cleanly distributed input to learn the overlying tendency within the data without distortion. The model I implemented for this interview was &lt;strong&gt;Facebook’s Prophet&lt;/strong&gt; and its &lt;a href=&quot;https://facebook.github.io/prophet/docs/outliers.html&quot;&gt;official documents recommends to remove the outliers since the model can handle missing values quite well&lt;/a&gt;. OK, I get it now.&lt;/p&gt;

&lt;p&gt;A transition takes effort, time, courage, and flexibility - understanding differences and embracing new rules. I didn’t get the job this time but lessons were priceless.&lt;/p&gt;
</description>
        <pubDate>Wed, 21 Oct 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/2020/dealing-with-outliers/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/dealing-with-outliers/</guid>
        
        <category>Job search</category>
        
        <category>Data preprocessing</category>
        
        <category>Statistics</category>
        
        <category>Data Analytics</category>
        
        
        <category>Interview</category>
        
      </item>
    
      <item>
        <title>Project: Mobile Transaction Fraud Detection</title>
        <description>&lt;p&gt;In this 2-week project, I aimed to try something completely different from what I practiced in the &lt;a href=&quot;https://soyhyoj.github.io/2020/project-research-trend-analysis/&quot;&gt;previous project&lt;/a&gt;. Instead of the text data, there were 6M rows of numbers. Rather than munging the data for purely personal interest, I built predictive models that can potentially address a business solution for mobile fraud detection.&lt;/p&gt;

&lt;p&gt;This was a collaborative project done by &lt;a href=&quot;https://github.com/mariagaya&quot;&gt;Maria Gayà Fiol&lt;/a&gt; and myself. The data we selected was &lt;a href=&quot;https://www.kaggle.com/ntnu-testimon/paysim1&quot;&gt;a simulated mobile money transaction log available on the Kaggle website&lt;/a&gt;, generated by &lt;a href=&quot;https://github.com/EdgarLopezPhD/PaySim&quot;&gt;PaySim&lt;/a&gt; based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an African country.
(Reference: E. A. Lopez-Rojas , A. Elmir, and S. Axelsson. “PaySim: A financial mobile money simulator for fraud detection”. In: The 28th European Modeling and Simulation Symposium-EMSS, Larnaca, Cyprus. 2016).&lt;/p&gt;

&lt;p&gt;Our goal was to understand the features closely related to fraudulent profit by exploratory data analysis and make good predictions on the unseen data based on our findings. To enhance prediction performance, we implemented different machine learning algorithms including logistic regression, random forest, XGBoost, and neural network.&lt;/p&gt;

&lt;p&gt;The main challenge of this data was to predict its highly unbalanced binary targets. The population of real fraudulent activity among the entire records was only 0.1%. Therefore, it was crucial to detect all possibly-fraud transactions, lowering the false-negative cases(=fail to detect real frauds; FN) even if it means to involve some false positives(=falsely asserting a case as a fraud when it’s not; FP).&lt;/p&gt;

&lt;p&gt;The benefit of this synthetic data is that analysts like us could conduct fraud analytics overcoming the lack of public access to private records. However, its simplicity in feature attributes easily led us to the overfitting of algorithms. As our observation, implementing the logistic regression was enough to detect 99% of the fraud transactions maintaining a relatively low FP rate(6.5%) when introduced ‘class_weight’ hyperparameter to compensate for the imbalance present in the dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/soyhyoj/FraudDetectionPaysim&quot;&gt;Check this repository where my part of the data analysis and modeling is available.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/project-fraud-detection.png&quot; alt=&quot;confusion-matrix&quot; class=&quot;img-responsive&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 19 Sep 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/2020/project-fraud-detection/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/project-fraud-detection/</guid>
        
        <category>Mobile</category>
        
        <category>Fraud detection</category>
        
        <category>Predictive Analysis</category>
        
        <category>Machine Learning</category>
        
        <category>CodeOp</category>
        
        
        <category>project</category>
        
      </item>
    
      <item>
        <title>Project: Research Trend Analysis</title>
        <description>&lt;p&gt;As a neuroscience geek who has recently moved on to the field of data science, it was very clear to me that the first personal analytics project had to be something related to my dear past experience of being a neuroscientist.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.jneurosci.org/&quot;&gt;The Journal of Neuroscience&lt;/a&gt;, a weekly peer-reviewed journal published by &lt;a href=&quot;https://www.sfn.org/&quot;&gt;the Society for Neuroscience&lt;/a&gt;, is probably one of the journals that I have read the most while designing, developing and conducting experiments.&lt;/p&gt;

&lt;p&gt;This 2-week sprint project was a perfect opportunity for me to leverage common NLP techniques in order to find an answer to my question:
Were the rising topics that I read as trends result of my attentional bias or was there a real fuss about them?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/project-research-trend.png&quot; alt=&quot;term-frequency&quot; class=&quot;img-responsive&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the beginning, this project was meant to be a cleaning-oriented one and I definitely enjoyed the wrangling part a lot. But the best part of this work is that I actually got to visualize some of the most stunning trends I’ve been observing these years: the efforts in academia to defeat the sexual bias prevalent in animal experiments&lt;br /&gt; (Scroll back and check the keywords including ‘male’ or ‘female’ from the graph above. What you see in the graph is the changes in the relative term frequency of indicated keywords during the last 10 years.)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3008499/&quot;&gt;Female mammals have long been neglected in biomedical research&lt;/a&gt; and not until recently did the investigators consider female samples for behavioral experiments in order to ensure the ‘consistency’ of their results. In 2012, I was actually advised not to include female animals for behavior studies since ‘female behaviors are unstable and it can only cause trouble when trying to publish the work’.&lt;/p&gt;

&lt;p&gt;Now the situation has changed. All the researchers have obligation to justify their choice when the study was based on a single-sex group (at least in the European research centers where I belonged). And this inclusion kicked in quite radically around the year 2016, as the result of this analysis suggests. Probably we cannot neglect the fact that this time window coincides with when &lt;a href=&quot;https://grants.nih.gov/grants/guide/notice-files/not-od-15-102.html&quot;&gt;the National Institutes of Health (NIH) grants started to ask for all applicants to design experiments accounting for Sex as a Biological Variable (SABV)&lt;/a&gt;. Now the scientists are strongly encouraged to involve both sexes in their study and the effort has been visible at least from my point of view.&lt;/p&gt;

&lt;p&gt;OK, so the most frequently appeared terms over the last 3-4 years have been &lt;strong&gt;‘female’ &amp;amp; ‘male’&lt;/strong&gt; with a reason. What else did I find from this data?&lt;/p&gt;

&lt;p&gt;Rather traditional subjects such as &lt;strong&gt;perception&lt;/strong&gt; (keywords: ‘hair cell’, ‘visual cortex’), &lt;strong&gt;decision making&lt;/strong&gt; (keyword: ‘prefrontal cortex’), &lt;strong&gt;memory&lt;/strong&gt; (keywords: ‘synaptic plasticity’, ‘working memory’) and &lt;strong&gt;brain activity monitoring&lt;/strong&gt; (keyword: ‘firing rate’, ‘action potential’, ‘magnetic resonance’, ‘neural activity’) were captured and they are actually steady-selling topics for over decades with some ebbs and flows.&lt;/p&gt;

&lt;p&gt;This project has some room for improvement. Some articles of particular releases were not collected due to API problems. Also, the final selection of terms can be refined by filtering out generic terms like ‘mechanism underlying’, ‘present study’, or ‘significance statement’ (these are not giving any information about the subjects). Lastly, I would like to investigate the possibility to aggregate the popular research terms by topics using modeling in NLP.&lt;/p&gt;

&lt;p&gt;For more details on how I processed this data: &lt;a href=&quot;https://github.com/soyhyoj/ResearchTrendAnalysis&quot;&gt;check my notebooks&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 17 Sep 2020 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/2020/project-research-trend-analysis/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/project-research-trend-analysis/</guid>
        
        <category>Neuroscience</category>
        
        <category>NLP</category>
        
        <category>Trend</category>
        
        <category>Analysis</category>
        
        <category>CodeOp</category>
        
        
        <category>project</category>
        
      </item>
    
  </channel>
</rss>
